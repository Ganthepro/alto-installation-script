version: '3.8'

services:
  # FastAPI service for ML prediction
  prediction_service:
    build:
      context: ./alto-ml-service/chiller_prediction
      dockerfile: Dockerfile
    image: chiller_prediction:${TAG:-latest}
    container_name: prediction-service
    restart: unless-stopped
    ports:
      - "8100:8100"
    volumes:
      - prediction_models:/app/models
      - mlruns:/mlflow/mlruns
    environment:
      - MODELS_DIR=/app/models
      - LOG_LEVEL=INFO
      - MLFLOW_TRACKING_URI=http://mlflow-ui:5000
      - GIT_PYTHON_REFRESH=quiet
    networks:
      - internal
    depends_on:
      - mlflow-ui

  # MLflow UI for tracking and visualizing ML experiments
  mlflow-ui:
    image: ghcr.io/mlflow/mlflow:v2.8.1
    container_name: mlflow-ui
    restart: unless-stopped
    ports:
      - "5000:5000"
    volumes:
      - mlflow_data:/mlflow
      - mlruns:/mlflow/mlruns
    environment:
      - MLFLOW_TRACKING_URI=http://localhost:5000
    command: >
      bash -c "mkdir -p /mlflow && 
      mlflow server 
      --host 0.0.0.0 
      --port 5000 
      --backend-store-uri sqlite:///mlflow/mlflow.db 
      --default-artifact-root /mlflow/mlruns"
    networks:
      - internal

volumes:
  prediction_models:
  mlruns:
  mlflow_data:

networks:
  internal:
    name: internal
